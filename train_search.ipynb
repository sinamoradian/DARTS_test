{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from model_search import Network\n",
    "from architect import Architect\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('--data', type=str, default='../data', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
    "parser.add_argument('--learning_rate_min', type=float, default=0.001, help='min learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='num of training epochs')\n",
    "parser.add_argument('--init_channels', type=int, default=16, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
    "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.3, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=2, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
    "parser.add_argument('--unrolled', action='store_true', default=False, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "\n",
    "CIFAR_CLASSES = 10\n",
    "\n",
    "\n",
    "def main():\n",
    "  if not torch.cuda.is_available():\n",
    "    logging.info('no gpu device available')\n",
    "    sys.exit(1)\n",
    "\n",
    "  np.random.seed(args.seed)\n",
    "  torch.cuda.set_device(args.gpu)\n",
    "  cudnn.benchmark = True\n",
    "  torch.manual_seed(args.seed)\n",
    "  cudnn.enabled=True\n",
    "  torch.cuda.manual_seed(args.seed)\n",
    "  logging.info('gpu device = %d' % args.gpu)\n",
    "  logging.info(\"args = %s\", args)\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  criterion = criterion.cuda()\n",
    "  model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\n",
    "  model = model.cuda()\n",
    "  logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "\n",
    "  optimizer = torch.optim.SGD(\n",
    "      model.parameters(),\n",
    "      args.learning_rate,\n",
    "      momentum=args.momentum,\n",
    "      weight_decay=args.weight_decay)\n",
    "\n",
    "  train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "  train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
    "\n",
    "  num_train = len(train_data)\n",
    "  indices = list(range(num_train))\n",
    "  split = int(np.floor(args.train_portion * num_train))\n",
    "\n",
    "  train_queue = torch.utils.data.DataLoader(\n",
    "      train_data, batch_size=args.batch_size,\n",
    "      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "      pin_memory=True, num_workers=2)\n",
    "\n",
    "  valid_queue = torch.utils.data.DataLoader(\n",
    "      train_data, batch_size=args.batch_size,\n",
    "      sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "      pin_memory=True, num_workers=2)\n",
    "\n",
    "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "  architect = Architect(model, args)\n",
    "\n",
    "  for epoch in range(args.epochs):\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    logging.info('epoch %d lr %e', epoch, lr)\n",
    "\n",
    "    genotype = model.genotype()\n",
    "    logging.info('genotype = %s', genotype)\n",
    "\n",
    "    print(F.softmax(model.alphas_normal, dim=-1))\n",
    "    print(F.softmax(model.alphas_reduce, dim=-1))\n",
    "\n",
    "    # training\n",
    "    train_acc, train_obj = train(train_queue, valid_queue, model, architect, criterion, optimizer, lr)\n",
    "    logging.info('train_acc %f', train_acc)\n",
    "\n",
    "    # validation\n",
    "    valid_acc, valid_obj = infer(valid_queue, model, criterion)\n",
    "    logging.info('valid_acc %f', valid_acc)\n",
    "\n",
    "    utils.save(model, os.path.join(args.save, 'weights.pt'))\n",
    "\n",
    "\n",
    "def train(train_queue, valid_queue, model, architect, criterion, optimizer, lr):\n",
    "  objs = utils.AvgrageMeter()\n",
    "  top1 = utils.AvgrageMeter()\n",
    "  top5 = utils.AvgrageMeter()\n",
    "\n",
    "  for step, (input, target) in enumerate(train_queue):\n",
    "    model.train()\n",
    "    n = input.size(0)\n",
    "\n",
    "    input = Variable(input, requires_grad=False).cuda()\n",
    "    target = Variable(target, requires_grad=False).cuda(async=True)\n",
    "\n",
    "    # get a random minibatch from the search queue with replacement\n",
    "    input_search, target_search = next(iter(valid_queue))\n",
    "    input_search = Variable(input_search, requires_grad=False).cuda()\n",
    "    target_search = Variable(target_search, requires_grad=False).cuda(async=True)\n",
    "\n",
    "    architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(input)\n",
    "    loss = criterion(logits, target)\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
    "    optimizer.step()\n",
    "\n",
    "    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "    objs.update(loss.data[0], n)\n",
    "    top1.update(prec1.data[0], n)\n",
    "    top5.update(prec5.data[0], n)\n",
    "\n",
    "    if step % args.report_freq == 0:\n",
    "      logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "  return top1.avg, objs.avg\n",
    "\n",
    "\n",
    "def infer(valid_queue, model, criterion):\n",
    "  objs = utils.AvgrageMeter()\n",
    "  top1 = utils.AvgrageMeter()\n",
    "  top5 = utils.AvgrageMeter()\n",
    "  model.eval()\n",
    "\n",
    "  for step, (input, target) in enumerate(valid_queue):\n",
    "    input = Variable(input, volatile=True).cuda()\n",
    "    target = Variable(target, volatile=True).cuda(async=True)\n",
    "\n",
    "    logits = model(input)\n",
    "    loss = criterion(logits, target)\n",
    "\n",
    "    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "    n = input.size(0)\n",
    "    objs.update(loss.data[0], n)\n",
    "    top1.update(prec1.data[0], n)\n",
    "    top5.update(prec5.data[0], n)\n",
    "\n",
    "    if step % args.report_freq == 0:\n",
    "      logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "  return top1.avg, objs.avg\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main() \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
