{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from operations import *\n",
    "from torch.autograd import Variable\n",
    "from genotypes import PRIMITIVES\n",
    "from genotypes import Genotype\n",
    "\n",
    "\n",
    "class MixedOp(nn.Module):\n",
    "\n",
    "  def __init__(self, C, stride):\n",
    "    super(MixedOp, self).__init__()\n",
    "    self._ops = nn.ModuleList()\n",
    "    for primitive in PRIMITIVES:\n",
    "      op = OPS[primitive](C, stride, False)\n",
    "      if 'pool' in primitive:\n",
    "        op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))\n",
    "      self._ops.append(op)\n",
    "\n",
    "  def forward(self, x, weights):\n",
    "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
    "\n",
    "\n",
    "class Cell(nn.Module):\n",
    "\n",
    "  def __init__(self, steps, multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
    "    super(Cell, self).__init__()\n",
    "    self.reduction = reduction\n",
    "\n",
    "    if reduction_prev:\n",
    "      self.preprocess0 = FactorizedReduce(C_prev_prev, C, affine=False)\n",
    "    else:\n",
    "      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0, affine=False)\n",
    "    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0, affine=False)\n",
    "    self._steps = steps\n",
    "    self._multiplier = multiplier\n",
    "\n",
    "    self._ops = nn.ModuleList()\n",
    "    self._bns = nn.ModuleList()\n",
    "    for i in range(self._steps):\n",
    "      for j in range(2+i):\n",
    "        stride = 2 if reduction and j < 2 else 1\n",
    "        op = MixedOp(C, stride)\n",
    "        self._ops.append(op)\n",
    "\n",
    "  def forward(self, s0, s1, weights):\n",
    "    s0 = self.preprocess0(s0)\n",
    "    s1 = self.preprocess1(s1)\n",
    "\n",
    "    states = [s0, s1]\n",
    "    offset = 0\n",
    "    for i in range(self._steps):\n",
    "      s = sum(self._ops[offset+j](h, weights[offset+j]) for j, h in enumerate(states))\n",
    "      offset += len(states)\n",
    "      states.append(s)\n",
    "\n",
    "    return torch.cat(states[-self._multiplier:], dim=1)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "\n",
    "  def __init__(self, C, num_classes, layers, criterion, steps=4, multiplier=4, stem_multiplier=3):\n",
    "    super(Network, self).__init__()\n",
    "    self._C = C\n",
    "    self._num_classes = num_classes\n",
    "    self._layers = layers\n",
    "    self._criterion = criterion\n",
    "    self._steps = steps\n",
    "    self._multiplier = multiplier\n",
    "\n",
    "    C_curr = stem_multiplier*C\n",
    "    self.stem = nn.Sequential(\n",
    "      nn.Conv2d(3, C_curr, 3, padding=1, bias=False),\n",
    "      nn.BatchNorm2d(C_curr)\n",
    "    )\n",
    " \n",
    "    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C\n",
    "    self.cells = nn.ModuleList()\n",
    "    reduction_prev = False\n",
    "    for i in range(layers):\n",
    "      if i in [layers//3, 2*layers//3]:\n",
    "        C_curr *= 2\n",
    "        reduction = True\n",
    "      else:\n",
    "        reduction = False\n",
    "      cell = Cell(steps, multiplier, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
    "      reduction_prev = reduction\n",
    "      self.cells += [cell]\n",
    "      C_prev_prev, C_prev = C_prev, multiplier*C_curr\n",
    "\n",
    "    self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "    self.classifier = nn.Linear(C_prev, num_classes)\n",
    "\n",
    "    self._initialize_alphas()\n",
    "\n",
    "  def new(self):\n",
    "    model_new = Network(self._C, self._num_classes, self._layers, self._criterion).cuda()\n",
    "    for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):\n",
    "        x.data.copy_(y.data)\n",
    "    return model_new\n",
    "\n",
    "  def forward(self, input):\n",
    "    s0 = s1 = self.stem(input)\n",
    "    for i, cell in enumerate(self.cells):\n",
    "      if cell.reduction:\n",
    "        weights = F.softmax(self.alphas_reduce, dim=-1)\n",
    "      else:\n",
    "        weights = F.softmax(self.alphas_normal, dim=-1)\n",
    "      s0, s1 = s1, cell(s0, s1, weights)\n",
    "    out = self.global_pooling(s1)\n",
    "    logits = self.classifier(out.view(out.size(0),-1))\n",
    "    return logits\n",
    "\n",
    "  def _loss(self, input, target):\n",
    "    logits = self(input)\n",
    "    return self._criterion(logits, target) \n",
    "\n",
    "  def _initialize_alphas(self):\n",
    "    k = sum(1 for i in range(self._steps) for n in range(2+i))\n",
    "    num_ops = len(PRIMITIVES)\n",
    "\n",
    "    self.alphas_normal = Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)\n",
    "    self.alphas_reduce = Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)\n",
    "    self._arch_parameters = [\n",
    "      self.alphas_normal,\n",
    "      self.alphas_reduce,\n",
    "    ]\n",
    "\n",
    "  def arch_parameters(self):\n",
    "    return self._arch_parameters\n",
    "\n",
    "  def genotype(self):\n",
    "\n",
    "    def _parse(weights):\n",
    "      gene = []\n",
    "      n = 2\n",
    "      start = 0\n",
    "      for i in range(self._steps):\n",
    "        end = start + n\n",
    "        W = weights[start:end].copy()\n",
    "        edges = sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[:2]\n",
    "        for j in edges:\n",
    "          k_best = None\n",
    "          for k in range(len(W[j])):\n",
    "            if k != PRIMITIVES.index('none'):\n",
    "              if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                k_best = k\n",
    "          gene.append((PRIMITIVES[k_best], j))\n",
    "        start = end\n",
    "        n += 1\n",
    "      return gene\n",
    "\n",
    "    gene_normal = _parse(F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy())\n",
    "    gene_reduce = _parse(F.softmax(self.alphas_reduce, dim=-1).data.cpu().numpy())\n",
    "\n",
    "    concat = range(2+self._steps-self._multiplier, self._steps+2)\n",
    "    genotype = Genotype(\n",
    "      normal=gene_normal, normal_concat=concat,\n",
    "      reduce=gene_reduce, reduce_concat=concat\n",
    "    )\n",
    "    return genotype\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
